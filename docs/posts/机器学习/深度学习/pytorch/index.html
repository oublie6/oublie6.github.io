<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>机器学习-深度学习-Pytorch - Oublie的Hugo博客</title><meta name="Description" content="Oublie的Hugo博客,hugo,golang,mysql,微服务"><meta property="og:title" content="机器学习-深度学习-Pytorch" />
<meta property="og:description" content="pytorch官网 pytorch教程 教程 tensors张量 张量是一种特殊的数据结构，与数组和矩阵非常相似 使用张量对模型的输入和输出以及模型的" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://oublie6.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" /><meta property="og:image" content="https://oublie6.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-09T16:22:35+08:00" />
<meta property="article:modified_time" content="2023-03-09T16:22:35+08:00" /><meta property="og:site_name" content="我的网站" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://oublie6.github.io/logo.png"/>

<meta name="twitter:title" content="机器学习-深度学习-Pytorch"/>
<meta name="twitter:description" content="pytorch官网 pytorch教程 教程 tensors张量 张量是一种特殊的数据结构，与数组和矩阵非常相似 使用张量对模型的输入和输出以及模型的"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="/images/%e5%a4%b4%e5%83%8f.jpeg"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://oublie6.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" /><link rel="prev" href="https://oublie6.github.io/posts/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/pandas/" /><link rel="next" href="https://oublie6.github.io/posts/%E6%9D%82%E9%A1%B9/%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "机器学习-深度学习-Pytorch",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/oublie6.github.io\/posts\/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\/pytorch\/"
        },"image": ["https:\/\/oublie6.github.io\/images\/%E5%A4%B4%E5%83%8F.jpeg"],"genre": "posts","keywords": "深度学习","wordcount":  3156 ,
        "url": "https:\/\/oublie6.github.io\/posts\/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\/pytorch\/","datePublished": "2023-03-09T16:22:35+08:00","dateModified": "2023-03-09T16:22:35+08:00","publisher": {
            "@type": "Organization",
            "name": "Oublie的Hugo博客","logo": "https:\/\/oublie6.github.io\/images\/%E5%A4%B4%E5%83%8F.jpeg"},"author": {
                "@type": "Person",
                "name": "oublie"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('light' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'light' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Oublie的Hugo博客">Oublie的Hugo博客</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Oublie的Hugo博客">Oublie的Hugo博客</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">机器学习-深度学习-Pytorch</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://oublie6.github.io/" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>oublie</a></span>&nbsp;<span class="post-category">included in <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>机器学习</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2023-03-09">2023-03-09</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;3156 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;7 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="true">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#教程">教程</a>
      <ul>
        <li><a href="#tensors张量">tensors张量</a>
          <ul>
            <li><a href="#初始化张量">初始化张量</a></li>
            <li><a href="#张量的属性">张量的属性</a></li>
            <li><a href="#张量运算">张量运算</a></li>
            <li><a href="#桥接-numpy">桥接 NumPy</a></li>
          </ul>
        </li>
        <li><a href="#数据集和数据加载器httpspytorchorgtutorialsbeginnerbasicsdata_tutorialhtml"><a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">数据集和数据加载器</a></a></li>
        <li><a href="#transforms变换">Transforms变换</a></li>
        <li><a href="#构建神经网络">构建神经网络</a>
          <ul>
            <li><a href="#设置训练设备">设置训练设备</a></li>
            <li><a href="#定义类">定义类</a></li>
            <li><a href="#模型层">模型层</a>
              <ul>
                <li><a href="#nnflatten">nn.Flatten</a></li>
                <li><a href="#nnlinear">nn.Linear</a></li>
                <li><a href="#nnrelu">nn.ReLU</a></li>
                <li><a href="#nnsequential">nn.Sequential</a></li>
                <li><a href="#nnsoftmax">nn.Softmax</a></li>
              </ul>
            </li>
            <li><a href="#模型参数">模型参数</a></li>
          </ul>
        </li>
        <li><a href="#自动微分torchautograd">自动微分TORCH.AUTOGRAD</a>
          <ul>
            <li><a href="#计算梯度">计算梯度</a></li>
            <li><a href="#禁用梯度跟踪">禁用梯度跟踪</a></li>
            <li><a href="#有关计算图的更多信息">有关计算图的更多信息</a></li>
          </ul>
        </li>
        <li><a href="#优化模型参数">优化模型参数</a>
          <ul>
            <li><a href="#超参数">超参数</a></li>
            <li><a href="#优化循环">优化循环</a></li>
            <li><a href="#损失函数">损失函数</a></li>
            <li><a href="#优化器">优化器</a></li>
          </ul>
        </li>
        <li><a href="#保存和加载模型">保存和加载模型</a>
          <ul>
            <li><a href="#保存和加载模型权重">保存和加载模型权重</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p><a href="https://pytorch.org/" target="_blank" rel="noopener noreffer ">pytorch官网</a></p>
<p><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreffer ">pytorch教程</a></p>
<h2 id="教程">教程</h2>
<h3 id="tensors张量">tensors张量</h3>
<p>张量是一种特殊的数据结构，与数组和矩阵非常相似</p>
<p>使用张量对模型的输入和输出以及模型的参数进行编码</p>
<p>导入张量</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">torch</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="初始化张量">初始化张量</h4>
<ol>
<li>直接来自已有数据:<code>x_data = torch.tensor([[1, 2],[3, 4]])</code></li>
<li>来自 NumPy 数组（也可以转换成NumPy数组）:</li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>np_array = np.array(data)
</span></span><span style="display:flex;"><span>x_np = torch.from_numpy(np_array)
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>从另一个张量（除非明确覆盖，否则新张量保留参数张量的属性（形状、数据类型））:</li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>x_ones = torch.ones_like(x_data) <span style="color:#aaa;font-style:italic"># retains the properties of x_data</span>
</span></span><span style="display:flex;"><span>x_rand = torch.rand_like(x_data, dtype=torch.float) <span style="color:#aaa;font-style:italic"># overrides the datatype of x_data</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>使用随机值或常数值</li>
</ol>
<p>shape是张量维度的元组:<code>shape = (2,3,)</code></p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>shape = (<span style="color:#099">2</span>,<span style="color:#099">3</span>,)
</span></span><span style="display:flex;"><span>rand_tensor = torch.rand(shape)
</span></span><span style="display:flex;"><span>ones_tensor = torch.ones(shape)
</span></span><span style="display:flex;"><span>zeros_tensor = torch.zeros(shape)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="张量的属性">张量的属性</h4>
<p>张量属性描述了它们的形状、数据类型和存储它们的设备</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>tensor = torch.rand(<span style="color:#099">3</span>,<span style="color:#099">4</span>)
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;Shape of tensor: </span><span style="color:#a50">{</span>tensor.shape<span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;Datatype of tensor: </span><span style="color:#a50">{</span>tensor.dtype<span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;Device tensor is stored on: </span><span style="color:#a50">{</span>tensor.device<span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="张量运算">张量运算</h4>
<p>所有的运算方式见<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreffer ">官网文档</a></p>
<p>默认情况下，张量是在 CPU 上创建的，使用.to方法将张量显式移动到 GPU</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00a">if</span> torch.cuda.is_available():
</span></span><span style="display:flex;"><span>    tensor = tensor.to(<span style="color:#a50">&#34;cuda&#34;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><ol>
<li>标准的类似 numpy 的索引和切片：</li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>tensor = torch.ones(<span style="color:#099">4</span>, <span style="color:#099">4</span>)
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;First row: </span><span style="color:#a50">{</span>tensor[<span style="color:#099">0</span>]<span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;First column: </span><span style="color:#a50">{</span>tensor[:, <span style="color:#099">0</span>]<span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;Last column: </span><span style="color:#a50">{</span>tensor[..., -<span style="color:#099">1</span>]<span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span><span style="display:flex;"><span>tensor[:,<span style="color:#099">1</span>] = <span style="color:#099">0</span>
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(tensor)
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>torch.cat沿给定维度连接一系列张量</li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>t1 = torch.cat([tensor, tensor, tensor], dim=<span style="color:#099">1</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>算术运算</li>
<li>单元素张量:使用聚合函数获取聚合值对象，然后使用聚合值对象的item()方法转换为python数值</li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>agg = tensor.sum()
</span></span><span style="display:flex;"><span>agg_item = agg.item()
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(agg_item, <span style="color:#0aa">type</span>(agg_item))
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>就地操作:将结果存储到操作数中的操作称为就地操作，这个方法会改变原张量，方法名以<code>_</code>符结尾</li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>tensor.add_(<span style="color:#099">5</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><p>会立即丢失历史记录，求导数可能会出问题，不鼓励使用</p>
<h4 id="桥接-numpy">桥接 NumPy</h4>
<p>张量的numpy方法转换为 NumPy 数组，这种转换类似于传指针，对张量的改变会等价改变numpy数组</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>t = torch.ones(<span style="color:#099">5</span>)
</span></span><span style="display:flex;"><span>n = t.numpy()
</span></span></code></pre></td></tr></table>
</div>
</div><p>NumPy 数组到 Tensor，对numpy数组的改变会等价改变张量</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>n = np.ones(<span style="color:#099">5</span>)
</span></span><span style="display:flex;"><span>t = torch.from_numpy(n)
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="数据集和数据加载器httpspytorchorgtutorialsbeginnerbasicsdata_tutorialhtml"><a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" target="_blank" rel="noopener noreffer ">数据集和数据加载器</a></h3>
<p>实现一个数据集类： <code>__init__</code>, <code>__len__</code>, 和 <code>__getitem__</code> 方法</p>
<h3 id="transforms变换">Transforms变换</h3>
<p>使用Transform对数据进行一些操作，使其适合训练。</p>
<ul>
<li>ToTensor():将 PIL 图像或 NumPy 转换ndarray为FloatTensor. 并在 [0., 1.] 范围内缩放图像的像素强度值</li>
<li>Lambda 转换:Lambda 转换应用任何用户定义的 lambda 函数</li>
</ul>
<h3 id="构建神经网络">构建神经网络</h3>
<p>神经网络由对数据执行操作的层/模块组成。torch.nn命名空间提供了构建自己的神经网络所需的所有构建块。</p>
<p>PyTorch 中的每个模块都是nn.Module 的子类。</p>
<p>神经网络本身就是一个由其他模块（层）组成的模块。</p>
<h4 id="设置训练设备">设置训练设备</h4>
<p>检查 torch.cuda 是否可用，不可用使用 CPU</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>device = <span style="color:#a50">&#34;cuda&#34;</span> <span style="color:#00a">if</span> torch.cuda.is_available() <span style="color:#00a">else</span> <span style="color:#a50">&#34;cpu&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="定义类">定义类</h4>
<p>通过子类化<code>nn.Module</code>来定义神经网络，在 <code>__init__</code> 方法中初始化神经网络层，在 forward 方法中实现对输入数据的操作，比如下面的NeuralNetwork类</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00a">class</span> <span style="color:#0a0;text-decoration:underline">NeuralNetwork</span>(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        <span style="color:#0aa">super</span>().__init__()
</span></span><span style="display:flex;"><span>        self.flatten = nn.Flatten()
</span></span><span style="display:flex;"><span>        self.linear_relu_stack = nn.Sequential(
</span></span><span style="display:flex;"><span>            nn.Linear(<span style="color:#099">28</span>*<span style="color:#099">28</span>, <span style="color:#099">512</span>),
</span></span><span style="display:flex;"><span>            nn.ReLU(),
</span></span><span style="display:flex;"><span>            nn.Linear(<span style="color:#099">512</span>, <span style="color:#099">512</span>),
</span></span><span style="display:flex;"><span>            nn.ReLU(),
</span></span><span style="display:flex;"><span>            nn.Linear(<span style="color:#099">512</span>, <span style="color:#099">10</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x = self.flatten(x)
</span></span><span style="display:flex;"><span>        logits = self.linear_relu_stack(x)
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> logits
</span></span></code></pre></td></tr></table>
</div>
</div><p>创建 NeuralNetwork 实例，并将其移动到device</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>model = NeuralNetwork().to(device)
</span></span></code></pre></td></tr></table>
</div>
</div><p>使用该模型只需将输入数据传递给它（使用__Call__方法），不要直接调用forward方法</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>X = torch.rand(<span style="color:#099">1</span>, <span style="color:#099">28</span>, <span style="color:#099">28</span>, device=device)
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic">## 直接调用__Call__方法</span>
</span></span><span style="display:flex;"><span>logits = model(X)
</span></span><span style="display:flex;"><span>pred_probab = nn.Softmax(dim=<span style="color:#099">1</span>)(logits)
</span></span><span style="display:flex;"><span>y_pred = pred_probab.argmax(<span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;Predicted class: </span><span style="color:#a50">{</span>y_pred<span style="color:#a50">}</span><span style="color:#a50">&#34;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="模型层">模型层</h4>
<p>分解 FashionMNIST 模型中的层，这里取3张28*28的图像</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>input_image = torch.rand(<span style="color:#099">3</span>,<span style="color:#099">28</span>,<span style="color:#099">28</span>)<span style="color:#aaa;font-style:italic"># [3, 28, 28]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="nnflatten">nn.Flatten</h5>
<p>将每个 2D 28x28 图像转换为 784 个像素值的连续数组</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>flatten = nn.Flatten()
</span></span><span style="display:flex;"><span>flat_image = flatten(input_image)<span style="color:#aaa;font-style:italic"># [3, 784]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="nnlinear">nn.Linear</h5>
<p>使用其存储的权重和偏差对输入应用线性变换</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>layer1 = nn.Linear(in_features=<span style="color:#099">28</span>*<span style="color:#099">28</span>, out_features=<span style="color:#099">20</span>)
</span></span><span style="display:flex;"><span>hidden1 = layer1(flat_image)<span style="color:#aaa;font-style:italic"># [3, 20]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="nnrelu">nn.ReLU</h5>
<p>在线性变换之后应用以引入非线性，帮助神经网络学习各种各样的现象</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>hidden1 = nn.ReLU()(hidden1)
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="nnsequential">nn.Sequential</h5>
<p>nn.Sequential是一个有序的模块容器，按照定义的相同顺序通过所有模块</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>seq_modules = nn.Sequential(
</span></span><span style="display:flex;"><span>    flatten,
</span></span><span style="display:flex;"><span>    layer1,
</span></span><span style="display:flex;"><span>    nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.Linear(<span style="color:#099">20</span>, <span style="color:#099">10</span>)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>input_image = torch.rand(<span style="color:#099">3</span>,<span style="color:#099">28</span>,<span style="color:#099">28</span>)
</span></span><span style="display:flex;"><span>logits = seq_modules(input_image)
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="nnsoftmax">nn.Softmax</h5>
<p>logits中值范围为[-infty, infty]，nn.Softmax将其缩放为值 [0, 1]，表示模型对每个类的预测概率，dim参数表示值总和必须为 1 的维度</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>softmax = nn.Softmax(dim=<span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>pred_probab = softmax(logits)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="模型参数">模型参数</h4>
<p>神经网络中的许多层都是参数化的，即具有在训练期间优化的相关权重和阈值。通过parameters()或named_parameters()方法访问</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00a">for</span> name, param <span style="color:#00a">in</span> model.named_parameters():
</span></span><span style="display:flex;"><span>    <span style="color:#0aa">print</span>(<span style="color:#a50">f</span><span style="color:#a50">&#34;Layer: </span><span style="color:#a50">{</span>name<span style="color:#a50">}</span><span style="color:#a50"> | Size: </span><span style="color:#a50">{</span>param.size()<span style="color:#a50">}</span><span style="color:#a50"> | Values : </span><span style="color:#a50">{</span>param[:<span style="color:#099">2</span>]<span style="color:#a50">}</span><span style="color:#a50"> </span><span style="color:#a50">\n</span><span style="color:#a50">&#34;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="自动微分torchautograd">自动微分TORCH.AUTOGRAD</h3>
<p>反向传播:根据损失函数相对于给定参数的梯度进行调整</p>
<p>torch.autograd支持自动计算任何计算图的梯度</p>
<div id="id-1"><figure><a class="lightgallery" href="https://pytorch.org/tutorials/_images/comp-graph.png" title="https://pytorch.org/tutorials/_images/comp-graph.png" data-thumbnail="https://pytorch.org/tutorials/_images/comp-graph.png" data-sub-html="<h2>例子</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://pytorch.org/tutorials/_images/comp-graph.png"
            data-srcset="https://pytorch.org/tutorials/_images/comp-graph.png, https://pytorch.org/tutorials/_images/comp-graph.png 1.5x, https://pytorch.org/tutorials/_images/comp-graph.png 2x"
            data-sizes="auto"
            alt="https://pytorch.org/tutorials/_images/comp-graph.png" />
    </a><figcaption class="image-caption">例子</figcaption>
    </figure></div>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">torch</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x = torch.ones(<span style="color:#099">5</span>)  <span style="color:#aaa;font-style:italic"># input tensor</span>
</span></span><span style="display:flex;"><span>y = torch.zeros(<span style="color:#099">3</span>)  <span style="color:#aaa;font-style:italic"># expected output</span>
</span></span><span style="display:flex;"><span>w = torch.randn(<span style="color:#099">5</span>, <span style="color:#099">3</span>, requires_grad=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span>b = torch.randn(<span style="color:#099">3</span>, requires_grad=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span>z = torch.matmul(x, w)+b
</span></span><span style="display:flex;"><span>loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)
</span></span></code></pre></td></tr></table>
</div>
</div><p>w和b是我们需要优化的参数，计算损失函数关于这些参数的的梯度，为此设置这些参数张量的requires_grad属性（定义张量时设置该属性为true或后面使用requires_grad_(True)方法设置）</p>
<p>张量的grad_fn属性为反向传播函数的引用</p>
<h4 id="计算梯度">计算梯度</h4>
<p>调用backward方法，当进行第二次以上重复调用时添加retain_graph=True参数给backward方法，而且多次重复调用会将结果累加，所以第二次以上调用时需要先将参数张量的grad属性归零</p>
<p>然后就可以检查设置了requires_grad为true的参数张量的grad属性，里面存储了计算的梯度</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>loss.backward()
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(w.grad)
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(b.grad)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="禁用梯度跟踪">禁用梯度跟踪</h4>
<p>对requires_grad=True的参数张量只进行正向计算不进行反向传播</p>
<ol>
<li>使用<code>with torch.no_grad():</code>块包围计算代码</li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00a">with</span> torch.no_grad():
</span></span><span style="display:flex;"><span>    z = torch.matmul(x, w)+b
</span></span><span style="display:flex;"><span><span style="color:#0aa">print</span>(z.requires_grad)
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>使用张量的detach方法的返回值</li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>z_det = z.detach()
</span></span></code></pre></td></tr></table>
</div>
</div><p>它的requires_grad为false</p>
<h4 id="有关计算图的更多信息">有关计算图的更多信息</h4>
<p>正向传递：</p>
<ol>
<li>运行请求的操作来计算结果张量</li>
<li>在 DAG 中维护操作的梯度函数</li>
</ol>
<p>DAG根上执行backward方法执行反向传播：</p>
<ol>
<li>根据grad_fn属性计算梯度</li>
<li>将结果存储在参数张量的grad属性中</li>
<li>使用链式法则，一直传播到叶张量</li>
</ol>
<h3 id="优化模型参数">优化模型参数</h3>
<p>训练模型是一个迭代过程；在每次迭代中，模型对输出进行猜测，计算其猜测中的误差（损失），收集关于其参数的误差的导数（如我们在上一节中看到的），并使用梯度下降优化这些参数.</p>
<h4 id="超参数">超参数</h4>
<p>超参数是可调整的参数，可让您控制模型优化过程。不同的超参数值会影响模型训练和收敛速度</p>
<ul>
<li>Number of Epochs：迭代数据集的次数</li>
<li>Batch Size：参数更新前通过网络传播的数据样本数量</li>
<li>Learning Rate：每个Batch/Epoch更新模型参数的步长。较小学习速度较慢的，较大可能会导致训练期间出现不可预测的行为</li>
</ul>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>learning_rate = <span style="color:#099">1e-3</span>
</span></span><span style="display:flex;"><span>batch_size = <span style="color:#099">64</span>
</span></span><span style="display:flex;"><span>epochs = <span style="color:#099">5</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="优化循环">优化循环</h4>
<p>优化循环的每次迭代称为一个epoch</p>
<p>每个epoch由两个主要部分组成：</p>
<ol>
<li>The Train Loop：迭代训练数据集并尝试收敛到最佳参数</li>
<li>The Validation/Test Loop：遍历测试数据集以检查模型性能是否正在提高</li>
</ol>
<h4 id="损失函数">损失函数</h4>
<p>损失函数衡量的是得到的结果与目标值的相异程度，训练目标是最小化的损失函数</p>
<p>使用给定数据样本的输入进行预测，损失函数将结果与真实数据标签值进行比较</p>
<p>常见的损失函数包括用于回归任务的nn.MSELoss（均方误差）和 用于分类的nn.NLLLoss（负对数似然）。nn.CrossEntropyLoss结合了nn.LogSoftmax和nn.NLLLoss</p>
<p>将模型的输出 logits 传递给nn.CrossEntropyLoss，这将标准化 logits 并计算预测误差</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># Initialize the loss function</span>
</span></span><span style="display:flex;"><span>loss_fn = nn.CrossEntropyLoss()
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="优化器">优化器</h4>
<p>优化算法（如SGD随机梯度下降，ADAM 和 RMSProp）定义了这个过程是如何执行的。所有优化逻辑都封装在optimizer对象中。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
</span></span></code></pre></td></tr></table>
</div>
</div><p>训练循环中，优化分三步进行</p>
<ol>
<li>调用optimizer.zero_grad()方法重置模型参数的梯度</li>
<li>调用loss.backward()方法反向传播预测损失</li>
<li>调用optimizer.step()方法通过反向传播的梯度调整参数张量</li>
</ol>
<h3 id="保存和加载模型">保存和加载模型</h3>
<h4 id="保存和加载模型权重">保存和加载模型权重</h4>
<p>torch.save函数保存张量的信息，可以只保存参数（model的state_dict方法），也可以保存包括形状的所有信息（依赖于pickle模块）</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>model = models.vgg16(pretrained=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># 保存参数信息</span>
</span></span><span style="display:flex;"><span>torch.save(model.state_dict(), <span style="color:#a50">&#39;model_weights.pth&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># 保存整个模型</span>
</span></span><span style="display:flex;"><span>torch.save(model, <span style="color:#a50">&#39;model.pth&#39;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><p>使用model.load_state_dict()方法加载模型参数，使用torch.load函数加载整个模型</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># 加载参数，运行推理之前，您必须调用model.eval()以将 dropout 和 batch normalization 层设置为评估模式</span>
</span></span><span style="display:flex;"><span>model = models.vgg16() <span style="color:#aaa;font-style:italic"># we do not specify pretrained=True, i.e. do not load default weights</span>
</span></span><span style="display:flex;"><span>model.load_state_dict(torch.load(<span style="color:#a50">&#39;model_weights.pth&#39;</span>))
</span></span><span style="display:flex;"><span>model.eval()
</span></span><span style="display:flex;"><span><span style="color:#aaa;font-style:italic"># 加载整个模型</span>
</span></span><span style="display:flex;"><span>model = torch.load(<span style="color:#a50">&#39;model.pth&#39;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2023-03-09
    

    
        

        
        
            <span id="busuanzi_container_value_page_pv"><i class="far fa-eye fa-fw"></i>
                
                <span id="busuanzi_value_page_pv"></span>&nbsp;views</span>
        
    

</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://oublie6.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" data-title="机器学习-深度学习-Pytorch" data-hashtags="深度学习"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://oublie6.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" data-hashtag="深度学习"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://oublie6.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" data-title="机器学习-深度学习-Pytorch"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://oublie6.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" data-title="机器学习-深度学习-Pytorch"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://oublie6.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch/" data-title="机器学习-深度学习-Pytorch"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/pandas/" class="prev" rel="prev" title="Python-第三方库-Pandas"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Python-第三方库-Pandas</a>
            <a href="/posts/%E6%9D%82%E9%A1%B9/%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0/" class="next" rel="next" title="杂项-个人笔记">杂项-个人笔记<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="gitalk" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://github.com/gitalk/gitalk"></a>Gitalk</a>.
            </noscript></div></article></div>
            </main>
    
        
        <script async src=" //busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js "></script>
    

    
        
            <section>
                
                    <span id="busuanzi_container_value_site_pv"><i class="far fa-eye fa-fw"></i>
                        
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                

                
                    &nbsp;|&nbsp;              
                

                
                    <span id="busuanzi_container_value_site_uv"><i class="fa fa-user"></i>
                        
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
            </section>
        

        
        
    

</div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><link rel="stylesheet" href="/css/d8d1a4.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{"gitalk":{"admin":["oublie6"],"clientID":"161f711b5310ee2521fa","clientSecret":"0bbbc4a0ed58220489334176a48a3c0898c1538a","id":"2023-03-09T16:22:35+08:00","owner":"oublie6","repo":"oublie6.github.io","title":"机器学习-深度学习-Pytorch"}},"lightgallery":true,"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
